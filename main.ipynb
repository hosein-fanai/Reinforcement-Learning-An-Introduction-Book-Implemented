{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToe:\n",
    "\n",
    "    def __init__(self, policy=None):\n",
    "        self.policy = policy\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=np.uint8)\n",
    "        self.state = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def check_board_is_full(self):\n",
    "        return (self.board == 0).sum() == 0\n",
    "\n",
    "    def calc_reward_and_done(self):\n",
    "        reward = -0.1\n",
    "        done = False\n",
    "\n",
    "        for i in range(3):\n",
    "            row = self.board[i].tolist()\n",
    "            if row == [1, 1, 1]:\n",
    "                reward += 10.1\n",
    "                done = True\n",
    "            elif row == [2, 2, 2]:\n",
    "                reward -= 10.\n",
    "                done = True\n",
    "\n",
    "        for j in range(3):\n",
    "            col = self.board[:, j].tolist()\n",
    "            if col == [1, 1, 1]:\n",
    "                reward += 10.1\n",
    "                done = True\n",
    "            elif col == [2, 2, 2]:\n",
    "                reward -= 10.\n",
    "                done = True\n",
    "\n",
    "        diag_main = [self.board[k, k] for k in range(3)]\n",
    "        diag_aux = [self.board[0, 2], self.board[1, 1], self.board[2, 0]]\n",
    "        if diag_main == [1, 1, 1] or diag_aux == [1, 1, 1]:\n",
    "            reward += 10.1\n",
    "            done = True\n",
    "        elif diag_main == [2, 2, 2] or diag_aux == [2, 2, 2]:\n",
    "            reward -= 10.\n",
    "            done = True\n",
    "\n",
    "        filled = self.check_board_is_full()\n",
    "        if filled and not done:\n",
    "            done = filled\n",
    "            reward -= 10.\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def calc_state(self):\n",
    "        self.state = np.sum([digit*pow(3, i) for i, digit in enumerate(self.board.flatten().tolist())])\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def take_action(self, action, act_as_O):\n",
    "        self.board = self.board.flatten()\n",
    "        self.board[action] = 1 if not act_as_O else 2\n",
    "        self.board = self.board.reshape((3, 3))\n",
    "\n",
    "        self.calc_state()\n",
    "\n",
    "    def play_as_opponent(self, self_play, self_play_type):\n",
    "        while self_play and (self.board == 0).sum() != 0:\n",
    "            if self_play_type == \"q_policy\":\n",
    "                opponent_action = self.policy.act_greedy_sampled(self.state)\n",
    "            elif self_play_type == \"random\":\n",
    "                opponent_action = np.random.randint(9)\n",
    "\n",
    "            if self.board.flatten()[opponent_action] == 0:\n",
    "                self.take_action(opponent_action, act_as_O=True)\n",
    "                break\n",
    "            else:\n",
    "                self_play_type = \"random\"\n",
    "\n",
    "    def step(self, action, self_play=True, self_play_type=\"random\", act_as_O=False):    \n",
    "        if self.board.flatten()[action] != 0: # 1 is X (as in the player) and 2 is O (as in the opponent)\n",
    "            return self.state, -1.1, False, {\"wrong input\": True}\n",
    "\n",
    "        self.take_action(action, act_as_O)\n",
    "\n",
    "        reward, done = self.calc_reward_and_done()\n",
    "\n",
    "        if not done:\n",
    "            self.play_as_opponent(self_play, self_play_type)\n",
    "\n",
    "        return self.state, reward, done, {\"wrong input\": False}\n",
    "\n",
    "    def render(self):\n",
    "        print('_'*25)\n",
    "        for i in range(3):\n",
    "            print('|'+' '*3, end=\"\")\n",
    "    \n",
    "            for j in range(3):\n",
    "                if self.board[i, j] == 0:\n",
    "                    print(' ', end=\"\")\n",
    "                elif self.board[i, j] == 1:\n",
    "                    print('X', end=\"\")\n",
    "                elif self.board[i, j] == 2:\n",
    "                    print('O', end=\"\")\n",
    "\n",
    "                if j != 2:\n",
    "                    print(' '*3+'|'+' '*3, end=\"\")\n",
    "            \n",
    "            print(' '*3+'|')\n",
    "            if i != 2:\n",
    "                print('|'+'_'*7+'|'+'_'*7+'|'+'_'*7+'|')\n",
    "        \n",
    "        print('|'+'_'*7+'|'+'_'*7+'|'+'_'*7+'|')\n",
    "        print()\n",
    "\n",
    "    def play_against_opponent(self, policy_fn):\n",
    "        state = self.reset()\n",
    "        info = {\"wrong input\": False}\n",
    "\n",
    "        while True:\n",
    "            if not info[\"wrong input\"]:\n",
    "                action = policy_fn(state)\n",
    "                _, _, done, _  = self.step(action, self_play=False)\n",
    "                self.render()\n",
    "                if done:\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Wrong choice! Choose again.\")\n",
    "\n",
    "            action_opponent = int(input(\"O's turn: \"))\n",
    "            state, _, done, info = self.step(action_opponent, self_play=False, act_as_O=True)\n",
    "            self.render()\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "\n",
    "    def __init__(self, table_shape=(3**9, 9)):\n",
    "        self.Q_table = np.zeros(table_shape)\n",
    "    \n",
    "    def save(self, path=\"./Policy.npy\"):\n",
    "        np.save(path, self.Q_table)\n",
    "\n",
    "    def load(self, path=\"./Policy.npy\"):\n",
    "        self.Q_table = np.load(path)\n",
    "\n",
    "    def train(self, env, n_epochs, max_steps=100, gamma=0.95, lr=0.1):\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            epsilon = max(0.01, 1 - epoch/(0.6*n_epochs))\n",
    "\n",
    "            state = env.reset()\n",
    "            rewards = 0.\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.act_epsilon_greedy(state, epsilon)\n",
    "\n",
    "                state_new, reward, done, info = env.step(action)\n",
    "\n",
    "                self.Q_table[state][action] += lr * (reward + gamma * self.Q_table[state_new].max() - self.Q_table[state][action])\n",
    "\n",
    "                state = state_new\n",
    "                rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\rEpoch: {epoch}, Epsilon: {epsilon:.4f}, Rewards: {rewards:.4f}\", end=\"\")\n",
    "\n",
    "    def act_greedy(self, state):\n",
    "        action = self.Q_table[state].argmax()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def act_epsilon_greedy(self, state, epsilon=0.01):\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = self.act_greedy(state)\n",
    "        else:\n",
    "            action = np.random.randint(9)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def act_greedy_sampled(self, state, temperature=0.5):\n",
    "        preds = self.Q_table[state]\n",
    "        exp_preds = np.exp(np.log(preds + np.abs(preds.min()) + 1e-5) / 0.5)\n",
    "        exp_preds /= np.sum(exp_preds) + 1e-7\n",
    "        action = np.random.multinomial(1, exp_preds, 1).argmax()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe(policy=q_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100000, Epsilon: 0.0100, Rewards: 9.800000"
     ]
    }
   ],
   "source": [
    "q_policy.train(env, n_epochs=100_000)\n",
    "q_policy.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.84111308, 8.01525486, 7.84942915, ..., 8.07641734, 7.75388114,\n",
       "        8.03591443],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_policy.load()\n",
    "q_policy.Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________\n",
      "|       |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |       |\n",
      "|_______|_______|_______|\n",
      "|       |       |       |\n",
      "|_______|_______|_______|\n",
      "\n",
      "_________________________\n",
      "|   O   |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |       |\n",
      "|_______|_______|_______|\n",
      "|       |       |       |\n",
      "|_______|_______|_______|\n",
      "\n",
      "_________________________\n",
      "|   O   |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |       |\n",
      "|_______|_______|_______|\n",
      "\n",
      "_________________________\n",
      "|   O   |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |   O   |\n",
      "|_______|_______|_______|\n",
      "\n",
      "_________________________\n",
      "|   O   |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |   X   |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |       |   O   |\n",
      "|_______|_______|_______|\n",
      "\n",
      "_________________________\n",
      "|   O   |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|       |   X   |   X   |\n",
      "|_______|_______|_______|\n",
      "|   O   |       |   O   |\n",
      "|_______|_______|_______|\n",
      "\n",
      "_________________________\n",
      "|   O   |       |   X   |\n",
      "|_______|_______|_______|\n",
      "|   X   |   X   |   X   |\n",
      "|_______|_______|_______|\n",
      "|   O   |       |   O   |\n",
      "|_______|_______|_______|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.play_against_opponent(policy_fn=q_policy.act_greedy_sampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('tf_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd007ac3b45ff6a667b1025d767f59ded49e869e4dbd8cb641dd50277aed6055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
